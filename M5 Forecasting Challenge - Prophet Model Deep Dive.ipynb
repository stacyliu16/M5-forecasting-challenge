{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Forecasting Challenge: Prophet Model\n",
    "#### Author: Stacy Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Prophet Model**: Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well. (https://facebook.github.io/prophet/)\n",
    "\n",
    "\n",
    "**The goal of Prophet Notebook**: This notebook will be used to predict sales using Prophet\n",
    "\n",
    "**The data**: We are working with **42,840 hierarchical time series**. The data were obtained in the 3 US states of California (CA), Texas (TX), and Wisconsin (WI). “Hierarchical” here means that data can be aggregated on different levels: item level, department level, product category level, and state level. The sales information reaches back from Jan 2011 to June 2016. In addition to the sales numbers, we are also given corresponding data on prices, promotions, and holidays. Note, that we have been warned that **most of the time series contain zero values.**\n",
    "\n",
    "The data comprises **3049** individual products from *3 categories* and *7 departments*, sold in 10 stores in 3 states. The hierachical aggregation captures the combinations of these factors. For instance, we can create 1 time series for all sales, 3 time series for all sales per state, and so on. The largest category is sales of all individual 3049 products per 10 stores for 30490 time series.\n",
    "\n",
    "The training data comes in the shape of 4 separate files:\n",
    "\n",
    "- sales_train.csv: this is our main training data.  Contains the historical daily unit sales data per product and store from d_1 - d_1913.\n",
    "\n",
    "- sell_prices.csv: the store and item IDs together with the sales price of the item as a weekly average.\n",
    "\n",
    "- calendar.csv: dates together with related features like day-of-the week, month, year, and an 3 binary flags for whether the stores in each state allowed purchases with SNAP food stamps at this date (1) or not (0).\n",
    "\n",
    "- sales_train_evaluation.csv - Available one month before the competition deadline. It will include sales from d_1 - d_1941.\n",
    "\n",
    "**The metrics:**\n",
    "\n",
    "The point forecast submission are being evaluated using the **Root Mean Squared Scaled Error (RMSSE)**, which is derived from the Mean Absolute Scaled Error (MASE) that was designed to be scale invariant and symmetric. \n",
    "\n",
    "**Model & Results:**\n",
    "\n",
    "Since at the item level data is fairly sporatic, I used a top down approach to predict 70 time series group by Department, Category, Store and State and then distribute down to the item level by weight (total sale in last 28 days). \n",
    "\n",
    "The following model metrics produced the smallest error of **0.57504** on the test set:\n",
    "- Prophet(growth='linear', holidays = holidays, uncertainty_samples=False, n_changepoints = 50, changepoint_prior_scale=0.7, changepoint_range=0.8, holidays_prior_scale=20, seasonality_mode='multiplicative', seasonality_prior_scale=20)\n",
    "    - Note that \"holidays\" includes events, snap dates, as well as a +2 and -3 window for events (from EDA we found that sales increased 2 days prior to event and decreases 3 days after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import os\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Skip to 'Apply Prophet' section for model testing**\n",
    "\n",
    "To speed up modeling, I'll be fitting the model and predicting sales for the top 100 revenue (sales price * sales) generating items from each category from the last 28 days (from 3/28/2016 to 4/24/2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below code to reduce memory usage when importing cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if   c_min > np.iinfo(np.int8 ).min and c_max < np.iinfo(np.int8).max :\n",
    "                    df[col] = df[col].astype(np.int8 )\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if   c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved sales data\n",
    "filename = os.path.join(\"sales.csv\")\n",
    "sales = reduce_mem_usage(pd.read_csv(filename))\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up modeling, I'll be fitting the model and predicting sales for the top 100 revenue (sales price * sales) generating items from each category from the last 28 days (from 3/28/2016 to 4/24/2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['date']=pd.to_datetime(sales['date'])\n",
    "\n",
    "# pull sales from the last 28 days of the training data\n",
    "sales_last28days = sales[sales['date']>=pd.to_datetime('20160328')]\n",
    "sales_last28days = sales_last28days[sales_last28days['date']<=pd.to_datetime('20160424')]\n",
    "\n",
    "# calculate revenue\n",
    "sales_last28days['revenue'] = sales_last28days['sales'] * sales_last28days['sell_price']\n",
    "\n",
    "# get dataframe sorted by revenue for each category \n",
    "top_100 = sales_last28days.groupby(['cat_id', 'id'], as_index=False).sum()\n",
    "top_100 = top_100.groupby(['cat_id']).apply(lambda x: x.sort_values([\"revenue\"], ascending = False)).reset_index(drop=True)\n",
    "# select top 100 rows within each category\n",
    "top_100 = top_100.groupby('cat_id').head(100)\n",
    "# pull the IDs\n",
    "top_100 = top_100['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "top_100.to_csv(\"top_100_items.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"top_100_items.csv\")\n",
    "top_100 = pd.read_csv(filename)\n",
    "\n",
    "# pull sales data of the top 100 items\n",
    "top_100_sales = sales.merge(top_100, on = ['id'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and save to file\n",
    "top_100_sales = top_100_sales.groupby(['id']).apply(lambda x: x.sort_values([\"date\"], ascending = True)).reset_index(drop=True)\n",
    "top_100_sales.to_csv(\"top_100_sales.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply simple Prophet on 300 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved top 100 sales data\n",
    "filename = os.path.join(\"top_100_sales.csv\")\n",
    "top_100_sales = pd.read_csv(filename)\n",
    "top_100_sales['date']=pd.to_datetime(top_100_sales['date'])\n",
    "top_100_sales.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing\n",
    "cut_off_date = pd.to_datetime('20160424')\n",
    "\n",
    "train = top_100_sales[top_100_sales['date']<=cut_off_date]\n",
    "test = top_100_sales[top_100_sales['date'] > cut_off_date]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many rows we have of each item\n",
    "train.id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to save results\n",
    "results1 = pd.DataFrame(columns=test['date'].unique())\n",
    "results1.insert(0, 'id', test['id'].unique())\n",
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.pivot(index='id', columns='date', values = 'sales').reset_index()\n",
    "\n",
    "print(dt.datetime.now())\n",
    "for i in range(len(train)):\n",
    "    df1 = train.loc[[i]].copy()\n",
    "    df1 = df1.drop(columns=['id'])\n",
    "    df1 = df1.T.reset_index().drop(columns=\"date\")\n",
    "    df1 = df1.rename(columns={i:\"y\"})\n",
    "    df1[\"ds\"] = pd.Series(pd.date_range('2011-01-29', periods=len(df1)))\n",
    "    df1 = df1[[\"ds\", \"y\"]]\n",
    "    model = Prophet(uncertainty_samples=False, daily_seasonality=True)\n",
    "    model.fit(df1)\n",
    "    future_data=model.make_future_dataframe(periods=28).tail(28)\n",
    "    forecast_data=model.predict(future_data)\n",
    "    results1.iloc[i,1:] = forecast_data['yhat'].iloc[-28:].values\n",
    "    if i == 300: \n",
    "       # print(sales_train.loc[[i], \"id\"])\n",
    "       # print(forecast_data[['ds', 'yhat']].head(28))\n",
    "        break\n",
    "\n",
    "#transform results for evaluation\n",
    "results1.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "results1_transposed = pd.melt(results1,\n",
    "             id_vars = ['id'],\n",
    "             value_vars = [col for col in results.columns if col.startswith(\"F\")],\n",
    "             var_name = \"d\",\n",
    "             value_name = \"sales\")\n",
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "MSE = mean_squared_error(y_true = test.sales.values, y_pred = results1_transposed.sales.values)\n",
    "RMSE = np.sqrt(mean_squared_error(test.sales.values, results1_transposed.sales.values))\n",
    "print(MSE)\n",
    "print(RMSE)\n",
    "\n",
    "# Mean Forecast Error\n",
    "forecast_errors = [test.sales.values[i]-results1_transposed.sales.values[i] for i in range(len(test))]\n",
    "bias = sum(forecast_errors) * 1.0/len(test)\n",
    "print('Bias: %f' % bias)\n",
    "\n",
    "#MAPE\n",
    "def percentage_error(actual, predicted):\n",
    "    res = np.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(test.sales.values, results1_transposed.sales.values)\n",
    "print('MAPE: %f' % mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline result:\n",
    "    - Model runtime: 7min\n",
    "    - MSE: 531.0445125981449\n",
    "    - RMSE: 23.044403064478477\n",
    "    - Bias: 0.675911\n",
    "    - MAPE: 258.782741\n",
    "Bias result is positive, meanng that we have under forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Christmas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_modified = train.copy()\n",
    "train_modified = train_modified.drop(columns=[pd.to_datetime('20111225'),\n",
    "                                             pd.to_datetime('20121225'),\n",
    "                                             pd.to_datetime('20131225'),\n",
    "                                             pd.to_datetime('20141225'),\n",
    "                                             pd.to_datetime('20151225'),], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Holiday & SNAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import calendar\n",
    "filename = os.path.join(\"Data\", \"m5-forecasting-accuracy-updated\", \"calendar.csv\")\n",
    "calendar = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>ds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SuperBowl</td>\n",
       "      <td>2011-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ValentinesDay</td>\n",
       "      <td>2011-02-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PresidentsDay</td>\n",
       "      <td>2011-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LentStart</td>\n",
       "      <td>2011-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LentWeek2</td>\n",
       "      <td>2011-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            holiday         ds\n",
       "0         SuperBowl 2011-02-06\n",
       "1     ValentinesDay 2011-02-14\n",
       "2     PresidentsDay 2011-02-21\n",
       "3         LentStart 2011-03-09\n",
       "4         LentWeek2 2011-03-16\n",
       "...             ...        ...\n",
       "1958        snap_WI 2016-06-09\n",
       "1960        snap_WI 2016-06-11\n",
       "1961        snap_WI 2016-06-12\n",
       "1963        snap_WI 2016-06-14\n",
       "1964        snap_WI 2016-06-15\n",
       "\n",
       "[2117 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Holiday\n",
    "calendar['ds'] = pd.to_datetime(calendar['date'])\n",
    "events1 = pd.Series(calendar['event_name_1'].values, index=calendar['ds'].values).dropna()\n",
    "events2 = pd.Series(calendar['event_name_2'].values, index=calendar['ds'].values).dropna()\n",
    "events = pd.DataFrame(pd.concat([events1, events2], axis=0))\n",
    "events['ds'] = events.index.values\n",
    "events.rename({0: 'holiday'}, axis=1, inplace=True)\n",
    "events.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#SNAP\n",
    "df_ev_3 = pd.DataFrame({'holiday': 'snap_CA', 'ds': calendar[calendar['snap_CA'] == 1]['date']})\n",
    "df_ev_4 = pd.DataFrame({'holiday': 'snap_TX', 'ds': calendar[calendar['snap_TX'] == 1]['date']})\n",
    "df_ev_5 = pd.DataFrame({'holiday': 'snap_WI', 'ds': calendar[calendar['snap_WI'] == 1]['date']})\n",
    "holidays = pd.concat((events, df_ev_3, df_ev_4, df_ev_5))\n",
    "\n",
    "holidays['ds'] = pd.to_datetime(holidays['ds'])\n",
    "\n",
    "holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model with tuned inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.datetime.now())\n",
    "for i in range(len(train_modified)):\n",
    "    df1 = train.loc[[i]].copy()\n",
    "    df1 = df1.drop(columns=['id'])\n",
    "    df1 = df1.T.reset_index().drop(columns=\"date\")\n",
    "    df1 = df1.rename(columns={i:\"y\"})\n",
    "    df1[\"ds\"] = pd.Series(pd.date_range('2011-01-29', periods=len(df1)))\n",
    "    df1 = df1[[\"ds\", \"y\"]]\n",
    "    model = Prophet(holidays = holidays, uncertainty_samples=False, \n",
    "                    daily_seasonality=True, n_changepoints = 50, \n",
    "                    changepoint_range = 0.8, changepoint_prior_scale = 0.7)\n",
    "    model.fit(df1)\n",
    "    future_data=model.make_future_dataframe(periods=28).tail(28)\n",
    "    forecast_data=model.predict(future_data)\n",
    "    results.iloc[i,1:] = forecast_data['yhat'].iloc[-28:].values\n",
    "    if i == 300: \n",
    "       # print(sales_train.loc[[i], \"id\"])\n",
    "       # print(forecast_data[['ds', 'yhat']].head(28))\n",
    "        break\n",
    "\n",
    "#transform results for evaluation\n",
    "results.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "results_transposed = pd.melt(results,\n",
    "             id_vars = ['id'],\n",
    "             value_vars = [col for col in results.columns if col.startswith(\"F\")],\n",
    "             var_name = \"d\",\n",
    "             value_name = \"sales\")\n",
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate result\n",
    "MSE = mean_squared_error(y_true = test.sales.values, y_pred = results_transposed.sales.values)\n",
    "RMSE = np.sqrt(mean_squared_error(test.sales.values, results_transposed.sales.values))\n",
    "print(MSE)\n",
    "print(RMSE)\n",
    "\n",
    "# Mean Forecast Error\n",
    "forecast_errors = [test.sales.values[i]-results_transposed.sales.values[i] for i in range(len(test))]\n",
    "bias = sum(forecast_errors) * 1.0/len(test)\n",
    "print('Bias: %f' % bias)\n",
    "\n",
    "#MAPE\n",
    "mape = mean_absolute_percentage_error(test.sales.values, results_transposed.sales.values)\n",
    "print('MAPE: %f' % mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result with Holiday & Snap and some model tuning: Similiar MSE/RMSE as the baseline model but lower bias\n",
    "    - Model run time: 47 min\n",
    "    - MSE: 542.2541423909882\n",
    "    - RMSE: 23.28635098917364\n",
    "    - Bias: 0.454468"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add window to calendar since we know from EDA that sales increase 1-2 days prior to events and decreases 3 days after**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holiday\n",
    "calendar['ds'] = pd.to_datetime(calendar['date'])\n",
    "events1 = pd.Series(calendar['event_name_1'].values, index=calendar['ds'].values).dropna()\n",
    "events2 = pd.Series(calendar['event_name_2'].values, index=calendar['ds'].values).dropna()\n",
    "events = pd.DataFrame(pd.concat([events1, events2], axis=0))\n",
    "events['ds'] = events.index.values\n",
    "events.rename({0: 'holiday'}, axis=1, inplace=True)\n",
    "events.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#add lower and upper window\n",
    "events = pd.DataFrame({\n",
    "  'holiday': events['holiday'],\n",
    "  'ds': events['ds'],\n",
    "  'lower_window': -2,\n",
    "  'upper_window': 3\n",
    "})\n",
    "\n",
    "#SNAP\n",
    "df_ev_3 = pd.DataFrame({'holiday': 'snap_CA', 'ds': calendar[calendar['snap_CA'] == 1]['date'], 'lower_window': 0, 'upper_window': 0})\n",
    "df_ev_4 = pd.DataFrame({'holiday': 'snap_TX', 'ds': calendar[calendar['snap_TX'] == 1]['date'], 'lower_window': 0, 'upper_window': 0})\n",
    "df_ev_5 = pd.DataFrame({'holiday': 'snap_WI', 'ds': calendar[calendar['snap_WI'] == 1]['date'], 'lower_window': 0, 'upper_window': 0})\n",
    "holidays = pd.concat((events, df_ev_3, df_ev_4, df_ev_5))\n",
    "\n",
    "holidays['ds'] = pd.to_datetime(holidays['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>ds</th>\n",
       "      <th>lower_window</th>\n",
       "      <th>upper_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SuperBowl</td>\n",
       "      <td>2011-02-06</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ValentinesDay</td>\n",
       "      <td>2011-02-14</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PresidentsDay</td>\n",
       "      <td>2011-02-21</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LentStart</td>\n",
       "      <td>2011-03-09</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LentWeek2</td>\n",
       "      <td>2011-03-16</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            holiday         ds  lower_window  upper_window\n",
       "0         SuperBowl 2011-02-06            -2             3\n",
       "1     ValentinesDay 2011-02-14            -2             3\n",
       "2     PresidentsDay 2011-02-21            -2             3\n",
       "3         LentStart 2011-03-09            -2             3\n",
       "4         LentWeek2 2011-03-16            -2             3\n",
       "...             ...        ...           ...           ...\n",
       "1958        snap_WI 2016-06-09             0             0\n",
       "1960        snap_WI 2016-06-11             0             0\n",
       "1961        snap_WI 2016-06-12             0             0\n",
       "1963        snap_WI 2016-06-14             0             0\n",
       "1964        snap_WI 2016-06-15             0             0\n",
       "\n",
       "[2117 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified holiday\n",
    "events_1 = pd.DataFrame({'holiday': 'Event 1', 'ds': calendar[calendar['snap_CA'] == 1]['date']})\n",
    "events_2 = pd.DataFrame({'holiday': 'Event 2', 'ds': calendar[calendar['snap_CA'] == 1]['date']})\n",
    "df_ev_3 = pd.DataFrame({'holiday': 'snap_CA', 'ds': calendar[calendar['snap_CA'] == 1]['date']})\n",
    "df_ev_4 = pd.DataFrame({'holiday': 'snap_TX', 'ds': calendar[calendar['snap_TX'] == 1]['date']})\n",
    "df_ev_5 = pd.DataFrame({'holiday': 'snap_WI', 'ds': calendar[calendar['snap_WI'] == 1]['date']})\n",
    "holidays_simplified = pd.concat((events_1, events_2, df_ev_3, df_ev_4, df_ev_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified holiday with window\n",
    "events_1 = pd.DataFrame({'holiday': 'Event 1', 'ds': calendar[calendar['snap_CA'] == 1]['date'], 'lower_window': -2, 'upper_window': 3})\n",
    "events_2 = pd.DataFrame({'holiday': 'Event 2', 'ds': calendar[calendar['snap_CA'] == 1]['date'], 'lower_window': -2, 'upper_window': 3})\n",
    "df_ev_3 = pd.DataFrame({'holiday': 'snap_CA', 'ds': calendar[calendar['snap_CA'] == 1]['date'], 'lower_window': 0, 'upper_window': 0})\n",
    "df_ev_4 = pd.DataFrame({'holiday': 'snap_TX', 'ds': calendar[calendar['snap_TX'] == 1]['date'], 'lower_window': 0, 'upper_window': 0})\n",
    "df_ev_5 = pd.DataFrame({'holiday': 'snap_WI', 'ds': calendar[calendar['snap_WI'] == 1]['date'], 'lower_window': 0, 'upper_window': 0})\n",
    "holidays_simplified_window = pd.concat((events_1, events_2, df_ev_3, df_ev_4, df_ev_5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model with upper and lower window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.datetime.now())\n",
    "for i in range(len(train_modified)):\n",
    "    df1 = train.loc[[i]].copy()\n",
    "    df1 = df1.drop(columns=['id'])\n",
    "    df1 = df1.T.reset_index().drop(columns=\"date\")\n",
    "    df1 = df1.rename(columns={i:\"y\"})\n",
    "    df1[\"ds\"] = pd.Series(pd.date_range('2011-01-29', periods=len(df1)))\n",
    "    df1 = df1[[\"ds\", \"y\"]]\n",
    "    model = Prophet(holidays = holidays, uncertainty_samples=False, \n",
    "                    daily_seasonality=True, n_changepoints = 50, \n",
    "                    changepoint_range = 0.8, changepoint_prior_scale = 0.7)\n",
    "    model.fit(df1)\n",
    "    future_data=model.make_future_dataframe(periods=28).tail(28)\n",
    "    forecast_data=model.predict(future_data)\n",
    "    results.iloc[i,1:] = forecast_data['yhat'].iloc[-28:].values\n",
    "    print(i)\n",
    "    if i == 300: \n",
    "       # print(sales_train.loc[[i], \"id\"])\n",
    "       # print(forecast_data[['ds', 'yhat']].head(28))\n",
    "        break\n",
    "\n",
    "#transform results for evaluation\n",
    "results.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "results_transposed = pd.melt(results,\n",
    "             id_vars = ['id'],\n",
    "             value_vars = [col for col in results.columns if col.startswith(\"F\")],\n",
    "             var_name = \"d\",\n",
    "             value_name = \"sales\")\n",
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate result\n",
    "MSE = mean_squared_error(y_true = test.sales.values, y_pred = results_transposed.sales.values)\n",
    "RMSE = np.sqrt(mean_squared_error(test.sales.values, results_transposed.sales.values))\n",
    "print(MSE)\n",
    "print(RMSE)\n",
    "\n",
    "# Mean Forecast Error\n",
    "forecast_errors = [test.sales.values[i]-results_transposed.sales.values[i] for i in range(len(test))]\n",
    "bias = sum(forecast_errors) * 1.0/len(test)\n",
    "print('Bias: %f' % bias)\n",
    "\n",
    "#MAPE\n",
    "mape = mean_absolute_percentage_error(test.sales.values, results_transposed.sales.values)\n",
    "print('MAPE: %f' % mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result with Holiday (with upper and lower window) & Snap and some model tuning: Higher MSE/RMSE as the baseline model and negative bias, meaning I've overforcasted \n",
    "\n",
    "- Model run time: 3hr15min\n",
    "- MSE: 601.0255994065685\n",
    "- RMSE: 24.515823449490096\n",
    "- Bias: -0.334648"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine result1 and result so that Holiday is only applied to food and household**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_food = results_transposed.iloc[0:2800]\n",
    "results_household = results_transposed.iloc[5600:8401]\n",
    "results1_hobbies = results1_transposed.iloc[2800:5600]\n",
    "results_combined = pd.concat((results_food, results_household, results1_hobbies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate result\n",
    "MSE = mean_squared_error(y_true = test.sales.values, y_pred = results_combined.sales.values)\n",
    "RMSE = np.sqrt(mean_squared_error(test.sales.values, results_combined.sales.values))\n",
    "print(MSE)\n",
    "print(RMSE)\n",
    "\n",
    "# Mean Forecast Error\n",
    "forecast_errors = [test.sales.values[i]-results_combined.sales.values[i] for i in range(len(test))]\n",
    "bias = sum(forecast_errors) * 1.0/len(test)\n",
    "print('Bias: %f' % bias)\n",
    "\n",
    "#mape\n",
    "mape = mean_absolute_percentage_error(test.sales.values, results_combined.sales.values)\n",
    "print('MAPE: %f' % mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result with Holiday (with upper and lower window) & Snap and some model tuning on ONLY food and household and baseline model for hobbies: similiar MSE/RMSE as the baseline model but much lower bias\n",
    "\n",
    "    - Model run time: N/A\n",
    "    - MSE: 564.4136245161772\n",
    "    - RMSE: 23.75739094505491\n",
    "    - Bias: 0.199190\n",
    "    - MAPE: 286.449053\n",
    "    \n",
    "--> **Tuned model takes too long to run. Try a top down approach to limit number of runs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Top down approach**\n",
    " - Predict 70 time series group by Department, Category, Store and State and then distribute down to the item level by weight (total sale in last 28 days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "\n",
       "[5 rows x 1919 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = os.path.join(\"Data\",'m5-forecasting-accuracy-updated',\"sales_train_validation.csv\")\n",
    "sales = pd.read_csv(filename)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['d_1', 'd_2', 'd_3', 'd_4', 'd_5', 'd_6', 'd_7', 'd_8', 'd_9', 'd_10',\n",
       "       ...\n",
       "       'd_1904', 'd_1905', 'd_1906', 'd_1907', 'd_1908', 'd_1909', 'd_1910',\n",
       "       'd_1911', 'd_1912', 'd_1913'],\n",
       "      dtype='object', length=1913)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_columns = sales.columns[sales.columns.str.contains(\"d_\")]\n",
    "date_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the dates\n",
    "dates_s = [pd.to_datetime(calendar.loc[calendar['d'] == str_date,'date'].values[0]) for str_date in date_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sum sales by dept and store\n",
    "df_sale_group_item = sales[np.hstack([['dept_id','store_id'],date_columns])].groupby(['dept_id','store_id']).sum()\n",
    "df_sale_group_item = df_sale_group_item.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTimeSeries(dept_id, store_id):\n",
    "    item_series =  df_sale_group_item[(df_sale_group_item.dept_id == dept_id) & (df_sale_group_item.store_id == store_id)]\n",
    "    dates = pd.DataFrame({'ds': dates_s}, index=range(len(dates_s)))\n",
    "    dates['y'] = item_series[date_columns].values[0].transpose()     \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prophet(dept_id, store_id):\n",
    "    timeserie = CreateTimeSeries(dept_id, store_id) \n",
    "    model = Prophet(growth='linear', \n",
    "                    holidays = holidays, \n",
    "                    uncertainty_samples=False, \n",
    "                    n_changepoints = 50, \n",
    "                    changepoint_prior_scale=0.7, \n",
    "                    changepoint_range=0.8, \n",
    "                    holidays_prior_scale=20, \n",
    "                    seasonality_mode='multiplicative', \n",
    "                    seasonality_prior_scale=20,\n",
    "                    daily_seasonality=False)\n",
    "   # model.add_seasonality(name='monthly', period=365.25/12, fourier_order=55)\n",
    "    # model.add_seasonality(name='quarterly', period=365.25/4, fourier_order=5)\n",
    "    model.fit(timeserie)\n",
    "    forecast = model.make_future_dataframe(periods=28, include_history=False)\n",
    "    forecast = model.predict(forecast)\n",
    "    return np.append(np.array([dept_id,store_id]),forecast['yhat'].values.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list param\n",
    "ids = []\n",
    "for i in range(0,df_sale_group_item.shape[0]):\n",
    "    ids = ids + [(df_sale_group_item[i:i+1]['dept_id'].values[0],df_sale_group_item[i:i+1]['store_id'].values[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in range(len(ids)):\n",
    "    results = list(run_prophet(ids[i][0],ids[i][1]))\n",
    "    predictions.append(results)\n",
    "    print(i)\n",
    "    if i == len(ids)+1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission\n",
    "df_sub_eva = pd.DataFrame()\n",
    "for k in range(0, len(predictions)):\n",
    "    dept_id = predictions[k][0]\n",
    "    store_id = predictions[k][1]\n",
    "\n",
    "    df_item = sales.loc[(sales.dept_id == dept_id) & (sales.store_id == store_id)][['id']]\n",
    "    df_item['val'] = sales[(sales.dept_id == dept_id) & (sales.store_id == store_id)].iloc[:, np.r_[0,-28:0]].sum(axis = 1)\n",
    "    for i in range(1,29):\n",
    "        df_item[f'F{i}'] = (df_item['val'] * float(predictions[k][i+1]) / df_item['val'].sum())\n",
    "    df_sub_eva = pd.concat([df_sub_eva, df_item])\n",
    "\n",
    "df_sub_eva = df_sub_eva.drop('val',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
       "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
       "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "2       0       2       3       0       1  \n",
       "3       1       3       0       2       6  \n",
       "4       0       0       2       1       0  \n",
       "\n",
       "[5 rows x 1947 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = os.path.join(\"Data\",'m5-forecasting-accuracy-updated',\"sales_train_evaluation.csv\")\n",
    "sales = pd.read_csv(filename)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['d_1', 'd_2', 'd_3', 'd_4', 'd_5', 'd_6', 'd_7', 'd_8', 'd_9', 'd_10',\n",
       "       ...\n",
       "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938',\n",
       "       'd_1939', 'd_1940', 'd_1941'],\n",
       "      dtype='object', length=1941)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_columns = sales.columns[sales.columns.str.contains(\"d_\")]\n",
    "date_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the dates\n",
    "dates_s = [pd.to_datetime(calendar.loc[calendar['d'] == str_date,'date'].values[0]) for str_date in date_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum sales by dept and store\n",
    "df_sale_group_item = sales[np.hstack([['dept_id','store_id'],date_columns])].groupby(['dept_id','store_id']).sum()\n",
    "df_sale_group_item = df_sale_group_item.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list param\n",
    "ids = []\n",
    "for i in range(0,df_sale_group_item.shape[0]):\n",
    "    ids = ids + [(df_sale_group_item[i:i+1]['dept_id'].values[0],df_sale_group_item[i:i+1]['store_id'].values[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "predictions_val = []\n",
    "for i in range(len(ids)):\n",
    "    results = list(run_prophet(ids[i][0],ids[i][1]))\n",
    "    predictions_val.append(results)\n",
    "    print(i)\n",
    "    if i == len(ids)+1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission\n",
    "df_sub_val = pd.DataFrame()\n",
    "for k in range(0, len(predictions_val)):\n",
    "    dept_id = predictions_val[k][0]\n",
    "    store_id = predictions_val[k][1]\n",
    "\n",
    "    df_item = sales.loc[(sales.dept_id == dept_id) & (sales.store_id == store_id)][['id']]\n",
    "    df_item['val'] = sales[(sales.dept_id == dept_id) & (sales.store_id == store_id)].iloc[:, np.r_[0,-28:0]].sum(axis = 1)\n",
    "    for i in range(1,29):\n",
    "        df_item[f'F{i}'] = (df_item['val'] * float(predictions_val[k][i+1]) / df_item['val'].sum())\n",
    "    df_sub_val = pd.concat([df_sub_val, df_item])\n",
    "\n",
    "df_sub_val = df_sub_val.drop('val',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat evaluation and validation rows\n",
    "submission = pd.concat((df_sub_eva, df_sub_val), sort=False)\n",
    "submission = submission.sort_values('id').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>0.748841</td>\n",
       "      <td>0.745762</td>\n",
       "      <td>0.749093</td>\n",
       "      <td>0.787551</td>\n",
       "      <td>0.901222</td>\n",
       "      <td>1.087791</td>\n",
       "      <td>1.021491</td>\n",
       "      <td>0.823680</td>\n",
       "      <td>0.578717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918364</td>\n",
       "      <td>1.007640</td>\n",
       "      <td>0.910747</td>\n",
       "      <td>0.735664</td>\n",
       "      <td>0.720100</td>\n",
       "      <td>0.717941</td>\n",
       "      <td>0.751455</td>\n",
       "      <td>0.994230</td>\n",
       "      <td>1.013263</td>\n",
       "      <td>0.830553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_001_CA_1_validation</td>\n",
       "      <td>1.021954</td>\n",
       "      <td>1.006281</td>\n",
       "      <td>1.000192</td>\n",
       "      <td>0.883486</td>\n",
       "      <td>1.069442</td>\n",
       "      <td>1.399348</td>\n",
       "      <td>1.304984</td>\n",
       "      <td>1.040742</td>\n",
       "      <td>1.142979</td>\n",
       "      <td>...</td>\n",
       "      <td>1.331049</td>\n",
       "      <td>1.558718</td>\n",
       "      <td>1.416950</td>\n",
       "      <td>1.137125</td>\n",
       "      <td>1.140048</td>\n",
       "      <td>1.151739</td>\n",
       "      <td>1.227277</td>\n",
       "      <td>1.425870</td>\n",
       "      <td>1.657224</td>\n",
       "      <td>1.512108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_001_CA_2_evaluation</td>\n",
       "      <td>0.554705</td>\n",
       "      <td>0.573625</td>\n",
       "      <td>0.599493</td>\n",
       "      <td>0.636138</td>\n",
       "      <td>0.721050</td>\n",
       "      <td>0.876143</td>\n",
       "      <td>0.720564</td>\n",
       "      <td>0.834046</td>\n",
       "      <td>0.717240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769586</td>\n",
       "      <td>0.936655</td>\n",
       "      <td>0.859664</td>\n",
       "      <td>0.621730</td>\n",
       "      <td>0.637832</td>\n",
       "      <td>0.661010</td>\n",
       "      <td>0.686456</td>\n",
       "      <td>0.773635</td>\n",
       "      <td>0.920755</td>\n",
       "      <td>0.801782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_001_CA_2_validation</td>\n",
       "      <td>0.851491</td>\n",
       "      <td>0.878169</td>\n",
       "      <td>0.913496</td>\n",
       "      <td>0.984459</td>\n",
       "      <td>1.137863</td>\n",
       "      <td>1.513834</td>\n",
       "      <td>1.306525</td>\n",
       "      <td>0.863447</td>\n",
       "      <td>0.785336</td>\n",
       "      <td>...</td>\n",
       "      <td>1.186151</td>\n",
       "      <td>1.524956</td>\n",
       "      <td>1.389607</td>\n",
       "      <td>0.941473</td>\n",
       "      <td>0.983431</td>\n",
       "      <td>1.034354</td>\n",
       "      <td>1.109787</td>\n",
       "      <td>1.273592</td>\n",
       "      <td>1.607171</td>\n",
       "      <td>1.471676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_001_CA_3_evaluation</td>\n",
       "      <td>0.877740</td>\n",
       "      <td>0.887814</td>\n",
       "      <td>0.886819</td>\n",
       "      <td>0.914589</td>\n",
       "      <td>0.953661</td>\n",
       "      <td>1.077233</td>\n",
       "      <td>1.128418</td>\n",
       "      <td>1.075661</td>\n",
       "      <td>0.794235</td>\n",
       "      <td>...</td>\n",
       "      <td>1.045239</td>\n",
       "      <td>1.148215</td>\n",
       "      <td>1.131104</td>\n",
       "      <td>0.874695</td>\n",
       "      <td>0.865258</td>\n",
       "      <td>0.872541</td>\n",
       "      <td>0.860069</td>\n",
       "      <td>1.051363</td>\n",
       "      <td>1.141122</td>\n",
       "      <td>1.026848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60975</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_1_validation</td>\n",
       "      <td>0.028825</td>\n",
       "      <td>0.028327</td>\n",
       "      <td>0.028833</td>\n",
       "      <td>0.025941</td>\n",
       "      <td>0.041314</td>\n",
       "      <td>0.046312</td>\n",
       "      <td>0.042362</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>0.027732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035626</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.039497</td>\n",
       "      <td>0.028692</td>\n",
       "      <td>0.028499</td>\n",
       "      <td>0.029318</td>\n",
       "      <td>0.030573</td>\n",
       "      <td>0.036836</td>\n",
       "      <td>0.045228</td>\n",
       "      <td>0.040877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60976</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_2_evaluation</td>\n",
       "      <td>0.061786</td>\n",
       "      <td>0.060831</td>\n",
       "      <td>0.062314</td>\n",
       "      <td>0.066730</td>\n",
       "      <td>0.075641</td>\n",
       "      <td>0.071002</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>0.061239</td>\n",
       "      <td>0.078423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080847</td>\n",
       "      <td>0.085555</td>\n",
       "      <td>0.077610</td>\n",
       "      <td>0.064538</td>\n",
       "      <td>0.060454</td>\n",
       "      <td>0.063132</td>\n",
       "      <td>0.067921</td>\n",
       "      <td>0.081902</td>\n",
       "      <td>0.083973</td>\n",
       "      <td>0.069620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60977</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_2_validation</td>\n",
       "      <td>0.030469</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.030985</td>\n",
       "      <td>0.033139</td>\n",
       "      <td>0.042432</td>\n",
       "      <td>0.047958</td>\n",
       "      <td>0.043909</td>\n",
       "      <td>0.032308</td>\n",
       "      <td>0.029081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038548</td>\n",
       "      <td>0.041075</td>\n",
       "      <td>0.037565</td>\n",
       "      <td>0.030258</td>\n",
       "      <td>0.029530</td>\n",
       "      <td>0.030157</td>\n",
       "      <td>0.032267</td>\n",
       "      <td>0.036778</td>\n",
       "      <td>0.041014</td>\n",
       "      <td>0.036889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60978</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_3_evaluation</td>\n",
       "      <td>0.063748</td>\n",
       "      <td>0.061603</td>\n",
       "      <td>0.060655</td>\n",
       "      <td>0.064715</td>\n",
       "      <td>0.079290</td>\n",
       "      <td>0.081049</td>\n",
       "      <td>0.068920</td>\n",
       "      <td>0.067294</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082487</td>\n",
       "      <td>0.090608</td>\n",
       "      <td>0.086524</td>\n",
       "      <td>0.065533</td>\n",
       "      <td>0.061737</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.064321</td>\n",
       "      <td>0.080629</td>\n",
       "      <td>0.080893</td>\n",
       "      <td>0.074558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60979</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60980 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id        F1        F2        F3  \\\n",
       "0          FOODS_1_001_CA_1_evaluation  0.748841  0.745762  0.749093   \n",
       "1          FOODS_1_001_CA_1_validation  1.021954  1.006281  1.000192   \n",
       "2          FOODS_1_001_CA_2_evaluation  0.554705  0.573625  0.599493   \n",
       "3          FOODS_1_001_CA_2_validation  0.851491  0.878169  0.913496   \n",
       "4          FOODS_1_001_CA_3_evaluation  0.877740  0.887814  0.886819   \n",
       "...                                ...       ...       ...       ...   \n",
       "60975  HOUSEHOLD_2_516_WI_1_validation  0.028825  0.028327  0.028833   \n",
       "60976  HOUSEHOLD_2_516_WI_2_evaluation  0.061786  0.060831  0.062314   \n",
       "60977  HOUSEHOLD_2_516_WI_2_validation  0.030469  0.030061  0.030985   \n",
       "60978  HOUSEHOLD_2_516_WI_3_evaluation  0.063748  0.061603  0.060655   \n",
       "60979  HOUSEHOLD_2_516_WI_3_validation  0.000000  0.000000  0.000000   \n",
       "\n",
       "             F4        F5        F6        F7        F8        F9  ...  \\\n",
       "0      0.787551  0.901222  1.087791  1.021491  0.823680  0.578717  ...   \n",
       "1      0.883486  1.069442  1.399348  1.304984  1.040742  1.142979  ...   \n",
       "2      0.636138  0.721050  0.876143  0.720564  0.834046  0.717240  ...   \n",
       "3      0.984459  1.137863  1.513834  1.306525  0.863447  0.785336  ...   \n",
       "4      0.914589  0.953661  1.077233  1.128418  1.075661  0.794235  ...   \n",
       "...         ...       ...       ...       ...       ...       ...  ...   \n",
       "60975  0.025941  0.041314  0.046312  0.042362  0.028047  0.027732  ...   \n",
       "60976  0.066730  0.075641  0.071002  0.059261  0.061239  0.078423  ...   \n",
       "60977  0.033139  0.042432  0.047958  0.043909  0.032308  0.029081  ...   \n",
       "60978  0.064715  0.079290  0.081049  0.068920  0.067294  0.070833  ...   \n",
       "60979  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  ...   \n",
       "\n",
       "            F19       F20       F21       F22       F23       F24       F25  \\\n",
       "0      0.918364  1.007640  0.910747  0.735664  0.720100  0.717941  0.751455   \n",
       "1      1.331049  1.558718  1.416950  1.137125  1.140048  1.151739  1.227277   \n",
       "2      0.769586  0.936655  0.859664  0.621730  0.637832  0.661010  0.686456   \n",
       "3      1.186151  1.524956  1.389607  0.941473  0.983431  1.034354  1.109787   \n",
       "4      1.045239  1.148215  1.131104  0.874695  0.865258  0.872541  0.860069   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "60975  0.035626  0.043900  0.039497  0.028692  0.028499  0.029318  0.030573   \n",
       "60976  0.080847  0.085555  0.077610  0.064538  0.060454  0.063132  0.067921   \n",
       "60977  0.038548  0.041075  0.037565  0.030258  0.029530  0.030157  0.032267   \n",
       "60978  0.082487  0.090608  0.086524  0.065533  0.061737  0.061458  0.064321   \n",
       "60979  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            F26       F27       F28  \n",
       "0      0.994230  1.013263  0.830553  \n",
       "1      1.425870  1.657224  1.512108  \n",
       "2      0.773635  0.920755  0.801782  \n",
       "3      1.273592  1.607171  1.471676  \n",
       "4      1.051363  1.141122  1.026848  \n",
       "...         ...       ...       ...  \n",
       "60975  0.036836  0.045228  0.040877  \n",
       "60976  0.081902  0.083973  0.069620  \n",
       "60977  0.036778  0.041014  0.036889  \n",
       "60978  0.080629  0.080893  0.074558  \n",
       "60979  0.000000  0.000000  0.000000  \n",
       "\n",
       "[60980 rows x 29 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv('prophet_final.csv', index = False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top down model tuning:\n",
    "1. Prophet(holidays = holidays_simplified, uncertainty_samples = False, n_changepoints = 50, changepoint_range = 0.8, changepoint_prior_scale = 0.7, seasonality_mode = 'multiplicative'): **0.64537**\n",
    "2. Prophet(holidays = holidays_simplified_window, uncertainty_samples = False, n_changepoints = 50, changepoint_range = 0.8, changepoint_prior_scale = 0.7, seasonality_mode = 'multiplicative'): **0.63286**\n",
    "3. Prophet(holidays = holidays, uncertainty_samples = False, n_changepoints = 50, changepoint_range = 0.8, changepoint_prior_scale = 0.7, seasonality_mode = 'multiplicative'): **0.57835**\n",
    "4. Prophet(growth='linear', holidays = holidays, uncertainty_samples=False, n_changepoints = 50, changepoint_prior_scale=30, changepoint_range=0.8, holidays_prior_scale=20, yearly_seasonality=2, daily_seasonality=False, weekly_seasonality=1, seasonality_mode='multiplicative', seasonality_prior_scale=30)              model.add_seasonality(name='monthly', period=365.25/12, fourier_order=55)               model.add_seasonality(name='quarterly', period=365.25/4, fourier_order=5): **0.62959**\n",
    "5. Prophet(growth='linear', holidays = holidays, uncertainty_samples=False, n_changepoints = 50, changepoint_prior_scale=0.7, changepoint_range=0.8, holidays_prior_scale=20, seasonality_mode='multiplicative', seasonality_prior_scale=20): **0.57504**\n",
    "6. Prophet(growth='linear', holidays = holidays, uncertainty_samples=False, n_changepoints = 50, changepoint_prior_scale=0.7, changepoint_range=0.8, holidays_prior_scale=20, seasonality_mode='multiplicative', seasonality_prior_scale=20) & LY top down distro (np.r_[0,-392:-364] instead of np.r_[0,-28:0]): **0.64049** --> revert back to last 28 days\n",
    "7. Prophet(growth='linear', holidays = holidays, uncertainty_samples=False, n_changepoints = 50, changepoint_prior_scale=0.9, changepoint_range=0.95, holidays_prior_scale=20, seasonality_mode='multiplicative', seasonality_prior_scale=20, daily_seasonality=False): **0.58784**\n",
    "8. Prophet(growth='linear', holidays = holidays, uncertainty_samples=False, n_changepoints = 50, changepoint_prior_scale=0.7, changepoint_range=0.8, holidays_prior_scale=20, seasonality_mode='multiplicative', seasonality_prior_scale=20) & **updated data based on EDA (for CA_3, removing d1510 to 1750, for TX, limit dataset to d1100 and after, for WI, limit dataset to d650 and after)**: **0.60384**\n",
    "9. Prophet(growth='linear', holidays = holidays, uncertainty_samples=False, n_changepoints = 50, changepoint_prior_scale=0.7, changepoint_range=0.8, holidays_prior_scale=20, seasonality_mode='multiplicative', seasonality_prior_scale=20) & **updated data based on EDA (for TX, limit dataset to d1100 and after, for WI, limit dataset to d650 and after)**: **0.60228**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify data based on EDA**\n",
    "- update data:\n",
    "    - for CA_3, removing d1510 to 1750\n",
    "    - for TX, limit dataset to d1100 and after\n",
    "    - for WI, limit dataset to d650 and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModifiedCreateTimeSeries(dept_id, store_id):\n",
    "    #for CA_3, removing d1510 to 1750\n",
    "    #if store_id == 'CA_3':\n",
    "        #df_sale_group_item_mod = df_sale_group_item.drop(df_sale_group_item.iloc[:,1512:1752], axis = 1)\n",
    "        #item_series =  df_sale_group_item_mod[(df_sale_group_item.dept_id == dept_id) & (df_sale_group_item.store_id == store_id)]\n",
    "        #dates_s_mod = dates_s[0:1510] + dates_s[1750:]\n",
    "        #dates = pd.DataFrame({'ds': dates_s_mod}, index=range(len(dates_s_mod)))\n",
    "        #date_columns_mod = date_columns[0:1510].append(date_columns[1750:])\n",
    "        #dates['y'] = item_series[date_columns_mod].values[0].transpose()     \n",
    "        #return dates\n",
    "    #for TX, limit dataset to d1100 and after\n",
    "    if store_id in set(['TX_1', 'TX_2', 'TX_3']):\n",
    "        df_sale_group_item_mod = df_sale_group_item.drop(df_sale_group_item.iloc[:,2:1101], axis = 1)\n",
    "        item_series =  df_sale_group_item_mod[(df_sale_group_item.dept_id == dept_id) & (df_sale_group_item.store_id == store_id)]\n",
    "        dates_s_mod = dates_s[1099:]\n",
    "        dates = pd.DataFrame({'ds': dates_s_mod}, index=range(len(dates_s_mod)))\n",
    "        date_columns_mod = date_columns[1099:]\n",
    "        dates['y'] = item_series[date_columns_mod].values[0].transpose()     \n",
    "        return dates\n",
    "    #for WI, limit dataset to d750 and after\n",
    "    if store_id in set(['WI_1', 'WI_2', 'WI_3']):\n",
    "        df_sale_group_item_mod = df_sale_group_item.drop(df_sale_group_item.iloc[:,2:751], axis = 1)\n",
    "        item_series =  df_sale_group_item_mod[(df_sale_group_item.dept_id == dept_id) & (df_sale_group_item.store_id == store_id)]\n",
    "        dates_s_mod = dates_s[749:]\n",
    "        dates = pd.DataFrame({'ds': dates_s_mod}, index=range(len(dates_s_mod)))\n",
    "        date_columns_mod = date_columns[749:]\n",
    "        dates['y'] = item_series[date_columns_mod].values[0].transpose()     \n",
    "        return dates\n",
    "    else:\n",
    "        item_series =  df_sale_group_item[(df_sale_group_item.dept_id == dept_id) & (df_sale_group_item.store_id == store_id)]\n",
    "        dates = pd.DataFrame({'ds': dates_s}, index=range(len(dates_s)))\n",
    "        dates['y'] = item_series[date_columns].values[0].transpose()     \n",
    "        return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_run_prophet(dept_id, store_id):\n",
    "    timeserie = ModifiedCreateTimeSeries(dept_id, store_id) \n",
    "    model = Prophet(growth='linear', \n",
    "                    holidays = holidays, \n",
    "                    uncertainty_samples=False, \n",
    "                    n_changepoints = 50, \n",
    "                    changepoint_prior_scale=0.7, \n",
    "                    changepoint_range=0.8, \n",
    "                    holidays_prior_scale=20, \n",
    "                    seasonality_mode='multiplicative', \n",
    "                    seasonality_prior_scale=20,\n",
    "                   daily_seasonality=False)\n",
    "    model.add_seasonality(name='monthly', period=365.25/12, fourier_order=55)\n",
    "    model.add_seasonality(name='quarterly', period=365.25/4, fourier_order=5)\n",
    "    model.fit(timeserie)\n",
    "    forecast = model.make_future_dataframe(periods=28, include_history=False)\n",
    "    forecast = model.predict(forecast)\n",
    "    return np.append(np.array([dept_id,store_id]),forecast['yhat'].values.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(len(ids)):\n",
    "    results = list(modified_run_prophet(ids[i][0],ids[i][1]))\n",
    "    predictions.append(results)\n",
    "    print(i)\n",
    "    if i == len(ids)+1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modified data** gave us a lower score -->revert back to originial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
